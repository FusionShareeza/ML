{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNN Linus Etemi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = 'GPU'\n",
    "\n",
    "if TYPE == 'CPU':\n",
    "    import numpy as np\n",
    "else:\n",
    "    import cupy as np\n",
    "    \n",
    "import scipy.special\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as matplot\n",
    "import pickle\n",
    "\n",
    "from ray import tune\n",
    "from csv import writer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hiddennodes_start = 800\n",
    "#hiddennodes_stop = 800\n",
    "#hiddennodes_step = 100\n",
    "\n",
    "#learningrate_start = 0.08\n",
    "#learningrate_stop = 0.08\n",
    "#learningrate_step = 0.01\n",
    "\n",
    "#epochs_start = 1\n",
    "#epochs_stop = 2\n",
    "#epochs_step = 0\n",
    "\n",
    "outputnodes = 10\n",
    "hiddennodes = 900\n",
    "learningrate = 0.05\n",
    "inputnodes = 784\n",
    "epochs= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronales Netz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    def __init__(self, innodes, outnodes, hidnodes, learningrate, epochs):\n",
    "        self.inodes = innodes\n",
    "        self.onodes = outnodes\n",
    "        self.hnodes = hidnodes\n",
    "        self.lr = learningrate\n",
    "        self.epo = epochs\n",
    "        self.wih = ((np.random.rand(self.hnodes,self.inodes)*2)-1)\n",
    "        self.who = ((np.random.rand(self.onodes,self.hnodes)*2)-1)\n",
    "        pass\n",
    "    \n",
    "    def activation_function(self, z: int) -> classmethod:\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        output_errors = targets - final_outputs\n",
    "        hidden_errors = np.dot(self.who.T, output_errors) \n",
    "        self.who += self.lr * np.dot( (output_errors*final_outputs*(1.0-final_outputs)), np.transpose(hidden_outputs)) \n",
    "        self.wih += self.lr * np.dot( (hidden_errors*hidden_outputs*(1.0-hidden_outputs)), np.transpose(inputs)) \n",
    "        pass\n",
    "\n",
    "    def query(self, input_list):\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        \n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_content(fname: str) -> None:\n",
    "    file = open(fname, \"w+\")\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(n, net, epo):  \n",
    "\n",
    "        clear_content('mnist_second_train.csv')\n",
    "        training_data_file = open(\"./MNISTData_red/mnist_train_red_8_9_by_90P.csv\", 'r')\n",
    "        training_data_list = training_data_file.readlines()\n",
    "        training_data_file.close()\n",
    "\n",
    "        for e in range(epo):\n",
    "            for record in training_data_list:\n",
    "                all_values = record.split(',')\n",
    "\n",
    "                inputs = (np.asarray(all_values[1:], dtype=np.float64) / 255.0 * 0.99) + 0.01\n",
    "                targets = np.zeros(outputnodes) + 0.01\n",
    "                targets[int(all_values[0])] = 0.99\n",
    "                n.train(inputs, targets)\n",
    "\n",
    "                pass\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_Network 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_network(n, net, epo):  \n",
    "#         if(net == 1):\n",
    "#             n.clear_content('mnist_second_train.csv')\n",
    "#             training_data_file = open(\"./MNISTData_red/mnist_train_red_8_9_by_90P.csv\", 'r')\n",
    "#             training_data_list = training_data_file.readlines()\n",
    "#             training_data_file.close()\n",
    "#         elif(net == 2):\n",
    "#             training_data_file = open(\"./mnist_second_train.csv\", 'r')\n",
    "#             training_data_list = training_data_file.readlines()\n",
    "#             training_data_file.close()\n",
    "#         for e in range(epo):\n",
    "#             for record in training_data_list:\n",
    "#                 all_values = record.split(',')\n",
    "\n",
    "#                 inputs = (np.asarray(all_values[1:], dtype=np.float64) / 255.0 * 0.99) + 0.01\n",
    "#                 targets = np.zeros(outputnodes) + 0.01\n",
    "#                 targets[int(all_values[0])] = 0.99\n",
    "#                 n.train(inputs, targets)\n",
    "\n",
    "#                 pass\n",
    "#             pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test_Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(n):\n",
    "\n",
    "    test_data_file = open(\"./MNISTData_red/mnist_train_red_8_9_by_90P.csv\", 'r')\n",
    "    test_data_list = test_data_file.readlines()\n",
    "    test_data_file.close()\n",
    "    array_data_all = []\n",
    "    scorecard_p = []    \n",
    "    scorecard_q = []\n",
    "\n",
    "    for i in range(100):\n",
    "        array_data_all.append([])\n",
    "        scorecard_p.append([])\n",
    "        scorecard_q.append([])\n",
    "\n",
    "    for record in test_data_list:\n",
    "        all_values = record.split(',')\n",
    "        correct_label = int(all_values[0])\n",
    "        inputs = (np.asarray(all_values[1:], dtype=np.float64) / 255.0 * 0.99) + 0.01\n",
    "        outputs = n.query(inputs)\n",
    "        label_max = outputs[np.argmax(outputs)]\n",
    "        label = np.argmax(outputs)\n",
    "\n",
    "        for threshold in range(100):\n",
    "            if(label_max > (threshold/100)):\n",
    "                scorecard_p[threshold].append(1)\n",
    "                if(label == correct_label):\n",
    "                    scorecard_q[threshold].append(1)\n",
    "                else:\n",
    "                    scorecard_q[threshold].append(0)\n",
    "            else:\n",
    "                scorecard_p[threshold].append(0)\n",
    "                #zweite netz werte \n",
    "\n",
    "    for i in range(100):\n",
    "        scorecard_p_array = np.asarray(scorecard_p[i])\n",
    "        scorecard_q_array = np.asarray(scorecard_q[i])\n",
    "\n",
    "        array_data_all[i].append(i/100)\n",
    "        array_data_all[i].append('P: {:.5f}'.format(scorecard_p_array.sum()/scorecard_p_array.size))\n",
    "        array_data_all[i].append('Q: {:.5f}'.format(scorecard_q_array.sum()/scorecard_q_array.size))\n",
    "\n",
    "\n",
    "    return array_data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def test_network2(n):\n",
    "    # test_data_file = open(\"./mnist_accepted.csv\", 'r')\n",
    "    # test_data_list = test_data_file.readlines()\n",
    "    # test_data_file.close()\n",
    "\n",
    "    # scorecard_train = []\n",
    "\n",
    "    # for record in test_data_list:\n",
    "\n",
    "    #         all_values = record.split(',')\n",
    "    #         correct_label = int(all_values[0])\n",
    "    #         inputs = (np.asarray(all_values[1:], dtype=np.float64) / 255.0 * 0.99) + 0.01\n",
    "    #         outputs = n.query(inputs)\n",
    "    #         score = outputs[np.argmax(outputs)]\n",
    "\n",
    "    #         if(np.argmax(outputs) == correct_label):\n",
    "    #             scorecard_train.append(1)\n",
    "    #         else:\n",
    "    #             scorecard_train.append(0)\n",
    "    #             append_list_as_row('mnist_second_train.csv', list(map(int, all_values)))\n",
    "                    \n",
    "\n",
    "\n",
    "    # scorecard_train_array = np.asarray(scorecard_train)\n",
    "    # performance2 = scorecard_train_array.sum() / scorecard_train_array.size\n",
    "\n",
    "    # return performance2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Writer und Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_list_as_row(fname,list_of_elem):\n",
    "    with open(fname, 'a+', newline='') as write_obj:\n",
    "        csv_writer = writer(write_obj)\n",
    "        csv_writer.writerow(list_of_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learningrate_i = learningrate_start\n",
    "#hiddennodes_i = hiddennodes_start\n",
    "#epochs_i = epochs_start\n",
    "\n",
    "\n",
    "#while epochs_i <= epochs_stop:\n",
    "#    while hiddennodes_i <= hiddennodes_stop:\n",
    "#        while learningrate_i <= learningrate_stop:\n",
    "#            intermediate_score = neuralNetwork(784,10,hiddennodes_i,learningrate_i,epochs_i).test_train()\n",
    "#            print('Epoch [{:02d}/{:02d}]     | Hidden_Nodes [{:04d}/{:04d}]     | Learning_Rate [{:.2f}/{:.2f}]     | Performance: {:.5f}'.format(epochs_i,epochs_stop,hiddennodes_i,hiddennodes_stop,learningrate_i,learningrate_stop,intermediate_score))\n",
    "#            append_list_as_row([784,10,hiddennodes_i,learningrate_i,epochs_i,intermediate_score])\n",
    "#            learningrate_i += learningrate_step \n",
    "#        learningrate_i = learningrate_start    \n",
    "#        hiddennodes_i += hiddennodes_step\n",
    "#        print(\"========================================================\")\n",
    "#    hiddennodes_i = hiddennodes_start\n",
    "#    epochs_i += epochs_step\n",
    "#    print(\"========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring und Aufruf von Beiden Netzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 1, 1], [0.01, 1, 1], [0.02, 1, 1], [0.03, 1, 1], [0.04, 1, 1], [0.05, 1, 1], [0.06, 1, 1], [0.07, 1, 1], [0.08, 1, 1], [0.09, 1, 1], [0.1, 1, 1], [0.11, 1, 1], [0.12, 1, 1], [0.13, 1, 1], [0.14, 1, 1], [0.15, 1, 1], [0.16, 1, 1], [0.17, 1, 1], [0.18, 1, 1], [0.19, 1, 1], [0.2, 1, 1], [0.21, 1, 1], [0.22, 1, 1], [0.23, 1, 1], [0.24, 1, 1], [0.25, 1, 1], [0.26, 1, 1], [0.27, 1, 1], [0.28, 1, 1], [0.29, 1, 1], [0.3, 1, 1], [0.31, 1, 1], [0.32, 1, 1], [0.33, 1, 1], [0.34, 1, 1], [0.35, 1, 1], [0.36, 1, 1], [0.37, 1, 1], [0.38, 1, 1], [0.39, 1, 1], [0.4, 1, 1], [0.41, 1, 1], [0.42, 1, 1], [0.43, 1, 1], [0.44, 1, 1], [0.45, 1, 1], [0.46, 1, 1], [0.47, 1, 1], [0.48, 1, 1], [0.49, 1, 1], [0.5, 1, 1], [0.51, 1, 1], [0.52, 1, 1], [0.53, 1, 1], [0.54, 1, 1], [0.55, 1, 1], [0.56, 1, 1], [0.57, 1, 1], [0.58, 1, 1], [0.59, 1, 1], [0.6, 1, 1], [0.61, 1, 1], [0.62, 1, 1], [0.63, 1, 1], [0.64, 1, 1], [0.65, 1, 1], [0.66, 1, 1], [0.67, 1, 1], [0.68, 1, 1], [0.69, 1, 1], [0.7, 1, 1], [0.71, 1, 1], [0.72, 1, 1], [0.73, 1, 1], [0.74, 1, 1], [0.75, 1, 1], [0.76, 1, 1], [0.77, 1, 1], [0.78, 1, 1], [0.79, 1, 1], [0.8, 1, 1], [0.81, 1, 1], [0.82, 1, 1], [0.83, 1, 1], [0.84, 1, 1], [0.85, 1, 1], [0.86, 1, 1], [0.87, 1, 1], [0.88, 1, 1], [0.89, 1, 1], [0.9, 1, 1], [0.91, 1, 1], [0.92, 1, 1], [0.93, 1, 1], [0.94, 1, 1], [0.95, 1, 1], [0.96, 1, 1], [0.97, 1, 1], [0.98, 1, 1], [0.99, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n1 = neuralNetwork(784,10,hiddennodes,learningrate,epochs)\n",
    "#train_network(n1, network_number, epochs)\n",
    "#pickle.dump(n1, open('neural_network_1.obj', 'wb'))\n",
    "n1 = pickle.load(open('neural_network_1.obj', 'rb'))\n",
    "\n",
    "intermediate_score1 = test_network(n1)\n",
    "print(intermediate_score1)\n",
    "\n",
    "\n",
    "# network_number=2\n",
    "# n2 = neuralNetwork(784,10,hiddennodes,learningrate,epochs)\n",
    "# train_network(n2, network_number, epochs)\n",
    "# intermediate_score2 = test_network(n2, network_number)\n",
    "# print(intermediate_score2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "t_values = []\n",
    "p_values = []\n",
    "q_values = []\n",
    "for i in range(100):\n",
    "    speicher = intermediate_score1[i]\n",
    "    t_values.append(speicher[0])\n",
    "    p_values.append(speicher[1])\n",
    "    q_values.append(speicher[2])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
